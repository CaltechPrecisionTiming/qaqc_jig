#!/usr/bin/env python3
"""
Creates charge histograms from hdf5 files made from `wavedump` or
`acquire-waveforms`.
"""

from __future__ import print_function, division
import h5py
import numpy as np
import pandas as pd
from scipy import signal
import os
import sys
from enum import Enum

canvas = []

class Institution(Enum):
    """
    Note: This must be kept in sync with the values in btl_qa.sql.
    """
    caltech = 'Caltech'
    uva = 'UVA'
    rome = 'Rome'

    def __str__(self):
        return self.value

def iqr(x):
    return np.percentile(x,75) - np.percentile(x,25)

def get_threshold_crossing(x, data, threshold=0.4, rising=True):
    """
    Returns the times at which the waveforms in `x` cross `threshold*100`% of
    their minimum value.
    
    WARNING: In cases where a pulse is cut off at the start/end of an event, this function produces a runtime warning.
    """
    data = np.asarray(data)
    argmin = np.argmin(data,axis=-1)
    thresholds = threshold*data[np.arange(data.shape[0]),argmin]
    if rising:
        il = data.shape[1]-np.argmax(((np.arange(data.shape[1]) < argmin[:,np.newaxis]) & (data > thresholds[:,np.newaxis]))[:,::-1],axis=-1)-1
        ir = il + 1
        ir[ir >= data.shape[1]] = data.shape[1]-1
        i = np.arange(data.shape[0])
    else:
        ir = np.argmax(((np.arange(data.shape[1]) > argmin[:,np.newaxis]) & (data > thresholds[:,np.newaxis])),axis=-1)
        il = ir - 1
        il[il < 0] = 0
        i = np.arange(data.shape[0])
    return x[il] + (thresholds-data[i,il])*(x[ir]-x[il])/(data[i,ir]-data[i,il])

def get_rise_time(x, data):
    t10 = get_threshold_crossing(x, data, 0.1)
    t90 = get_threshold_crossing(x, data, 0.9)
    return t90 - t10

def get_fall_time(x, data):
    t10 = get_threshold_crossing(x, data, 0.1, rising=False)
    t90 = get_threshold_crossing(x, data, 0.9, rising=False)
    return t10 - t90

def get_times(x, data, baseline=10):
    """
    Returns the times at which the waveforms in `x` cross 40% of their minimum
    value.
    """
    data = np.asarray(data)
    # Get the first 10 ns of every waveform to calculate the noise level
    noise = iqr(data[:,np.where(x < x[0] + baseline)[0]])
    # Get events with a pulse
    pulses = np.min(data,axis=-1) < -noise*5
    # Select events with pulses. If there are no matches (which might be the
    # case for the triggering channel), then don't apply the selection.
    if np.count_nonzero(pulses):
        data = data[pulses]
    argmin = np.argmin(data,axis=-1)
    threshold = 0.4*data[np.arange(data.shape[0]),argmin]
    return x[data.shape[1]-np.argmax(((np.arange(data.shape[1]) < argmin[:,np.newaxis]) & (data > threshold[:,np.newaxis]))[:,::-1],axis=-1)-1]

def get_window(x, data, left=1, right=10):
    """
    Returns the indices start and stop over which you should integrate the
    waveforms in `x`. The window is found by calculating the median hit time
    for all pulses in `x` and then going back `left` ns and forward `right` ns.
    """
    data = np.asarray(data)
    t = get_times(x,data)
    mean_hit_time = np.median(t)
    a, b = np.searchsorted(x,[mean_hit_time-left,mean_hit_time+right])
    if a < 0:
        a = 0
    if b > len(x) - 1:
        b = len(x) - 1
    return a, b

def get_spe_window(x, start, integration_time):
    """
    Returns the indicies over which the SPE analysis should be integrated.

    `start` and `integration_time` are in nanoseconds, not indexes.
    `start` is relative to the trigger, so `start = 0` is not the first sample,
    but rather the time the trigger fired.
    """
    time_per_index = x[1] - x[0]
    a = int(np.abs(x - start).argmin())
    if a >= len(x) - 1:
        print('SPE integration start time exceeds the acquisition window! Quitting...', file=sys.stderr)
        sys.exit(1)
    b = int(np.round(a + integration_time / time_per_index))
    if b > len(x) - 1:
        print('SPE integration time is too long! Quitting...', file=sys.stderr)
        sys.exit(1)
    return (a,b)

def integrate(x, data, a, b):
    """
    Integrate all waveforms in `data` with times `x`.
    """
    # i = v/r
    # divide by 50 ohms to convert to a charge
    if np.ndim(data) == 2:
        return -np.trapz(data[:,a:b],x=x[a:b])*1000/50.0
    else:
        return -np.trapz(data[a:b],x=x[a:b])*1000/50.0

def get_bins(x):
    """
    Returns bins for the data `x` using the Freedman Diaconis rule. See
    https://en.wikipedia.org/wiki/Freedman%E2%80%93Diaconis_rule.
    
    The equation below returns bins using said rule. A smaller bin value than
    this was chosen after finetuning for better fitting results.
    """
    
    bin_width = 0.5*iqr(x)/(len(x)**(1/3.0))
    if bin_width == 0:
        print('Zero bin width! Quitting...', file=sys.stderr)
        sys.exit(1)
    return np.arange(np.min(x),np.max(x),bin_width)

def chunks(lst, n):
    """
    Yield successive n-sized chunks from lst.
    """
    for i in range(0, len(lst), n):
        yield (i,i + n)

def convert_data(f, group, channel, start, stop):
    """
    Reads data from opened hdf5 file `f`. Gets the events from `start` to
    `stop` in the dataset `channel`.
    """    
    if 'data_source' in f[group].attrs:
        if f[group].attrs['data_source'] == b'CAEN':
            
            xinc = 1/(f[group].attrs['drs4_frequency'] * 10**6)
            points = f[group].attrs['record_length']
            x = np.linspace(0, xinc * points, int(points)) - xinc * points * (1 - f[group].attrs['post_trigger']/100)
            
            # While `y` is measured in volts, it's only relatively. We could
            # use the DC offset to determine the absolute voltage, but the DC
            # offset isn't well defined. Setting it to about 22000 (DAC units)
            # means approximately no offset is added, but not exactly.  This
            # shouldn't matter much because we use a baseline subtraction
            # method anyways.
            y = f[group][channel][start:stop] * (1/2**12)

    elif 'yinc' in dict(f[channel].attrs):
        x = f[channel].attrs['xorg'] + np.linspace(0,f[channel].attrs['xinc']*f[channel].attrs['points'],int(f[channel].attrs['points']))
        # FIXME
        # I believe the else block in this if/else statement was for a type of hdf5 format that we no longer use.
        if True: # ':WAVeform:FORMat' in dict(f['settings'].attrs) and f['settings'].attrs[':WAVeform:FORMat'] != 'ASC':
            # convert word values -> voltages if the data was saved in a non-ascii format
            y = f[channel][start:stop]*f[channel].attrs['yinc'] + f[channel].attrs['yorg']
        else:
            y = f[channel][start:stop]
    else:
        # In older versions of the code, I stored xorg, xinc, etc.
        # in the main HDF5 group and not on a per channel basis
        x = f.attrs['xorg'] + np.linspace(0,f.attrs['xinc']*f.attrs['points'],int(f.attrs['points']))

        if ':WAVeform:FORMat' in dict(f['settings'].attrs) and f['settings'].attrs[':WAVeform:FORMat'] != 'ASC':
            # convert word values -> voltages if the data was saved in a non-ascii format
            y = f[channel][start:stop]*f.attrs['yinc'] + f.attrs['yorg']
        else:
            y = f[channel][start:stop]
    return x*1e9, y

def low_filter_SPE(x, y):
    """
    Returns `y` through a low pass filter. Edit the cutoff frequency by
    modifying the filter defined below.
    """
    filter_order = 2
    nyquist = (0.5 * (x[1] - x[0]))**(-1)
    cutoff = 5**(-1)
    b, a = signal.butter(filter_order, min(1, cutoff/nyquist), btype='lowpass', output='ba')
    filter_data = signal.lfilter(b, a, y)
    return filter_data

def high_filter_SPE(x, y):
    """
    Returns `y` through a high pass filter. Edit the cutoff frequency by
    modifying the filter defined below.
    """
    filter_order = 2
    nyquist = (0.5 * (x[1] - x[0]))**(-1)
    cutoff = 5**(-1)  # Cut off frequency for the filter measured in inverse nanoseconds
    b, a = signal.butter(filter_order, min(1, cutoff/nyquist), btype='highpass', output='ba')
    filter_data = signal.lfilter(b, a, y)
    return filter_data

def spe_baseline_subtraction(x, y, method=1):
    """ 
    INTEGRATION METHODS
    0: Only per event median subtraction (preformed in every method).
    1 (default): For each event, subtract off the median of all points that lie above `cutoff`.
                 Then, across all events, subtract off the median of all points between `a` and `b`.
    2: Per sample median subtraction. This method is not good because the SPE signal gets diminished.
    3: Delete all trials that have an SPE between `ma` and `mb`. Subtract off the median between `ma` and `mb` on a per event basis.
    4: Same as 3, except no trials are deleted. Trials that have an SPE between `ma` and `mb` get reduced by the total median
       between `ma` and `mb` of events that don't have an SPE in this range.
    """ 
    high_filter_y = high_filter_SPE(x, y)
    
    # Integration Method 0, per event median subtraction:
    y -= np.median(y, axis=-1)[:, np.newaxis]
    
    if args.integration_method == 1:  # Default 
        cutoff = -2*iqr(high_filter_y.flatten())
        no_SPE_mask = y > cutoff 
        y -= np.array([np.median(y[i, no_SPE_mask[i]]) for i in range(len(y))])[:, np.newaxis]
        i_mask = np.logical_and(x>=args.start_time, x<args.start_time+args.integration_time)
        y -= np.median(y[:, i_mask])
    elif args.integration_method == 2:
        # `s` for samples
        s = y.T
        s_mask = s > -10*iqr(high_filter_y.flatten())
        good_s = [s[i, s_mask[i]] for i in range(len(s))]
        print(f'average number of good samples: {np.mean([len(sub) for sub in good_s])}')
        sample_medians = np.array([np.median(sub) for sub in good_s])
        y -= sample_medians
    elif args.integration_method == 3:
        ma = -25
        mb = 200
        SPE_trials = np.min(y[:, np.logical_and(x >= ma, x < mb)], axis=-1) < -2 * iqr(high_filter_y[:, np.logical_and(x >= ma, x < mb)].flatten())
        SPE_trials_idx = [i for i in range(len(y)) if SPE_trials[i]]
        y = np.delete(y, SPE_trials_idx, axis=0)
        # Subtract off the median between `ma` and `mb` per event
        y -= np.median(y[:, np.logical_and(x >= ma, x < mb)], axis=-1)[:, np.newaxis]  # per event median subtraction
        if len(y) == 0:
            print('All trials were removed')
    elif args.integration_method == 4:
        ma = -25
        mb = 200
        m_mask = np.logical_and(x>=ma, x<mb)
        no_SPE_trials_mask = np.min(y[:, m_mask], axis=-1) > -2 * iqr(high_filter_y[:, m_mask].flatten())
        y -= np.array([np.median(y[i, m_mask]) if no_SPE_trials_mask[i] else np.median((y[no_SPE_trials_mask, :])[:, m_mask]) for i in range(len(y))])[:, np.newaxis]
    elif args.integration_method != 0:
        print('Not a valid integration method. Defaulting to integration method 0')
    return (y, high_filter_y)

def plot_time_volt(x, y, channel, data_type, a, b, avg_y=None, pdf=False, filename=None):
    plt.figure()
    plt.subplot(2,1,1)
    plt.plot(x,y[:100].T)
    plt.xlabel("Time (ns)")
    plt.ylabel("Voltage (V)")
    plt.axvline(x[a])
    plt.axvline(x[b])
    plt.subplot(2,1,2)
    if avg_y is not None:
        plt.plot(x,avg_y)
    else:
        plt.plot(x, np.median(y, axis=0))
    plt.xlabel("Time (ns)")
    plt.ylabel("Voltage (V)")
    plt.axvline(x[a])
    plt.axvline(x[b])
    plt.suptitle(f"{data_type} {channel}")
    if args.print_pdfs:
        if not filename:
            print('No filename specified; can not print pdf!')
        else:
            root, ext = os.path.splitext(filename)
            plt.savefig(os.path.join(args.print_pdfs, f"{root}_{data_type}_{channel}_TimeVolt.pdf"))

def plot_hist(h, pdf=False, filename=None):
    global canvas
    # Naming canvases this way will produce a runtime warning because ROOT
    # will always make a default canvas with name `c1` the first time you
    # fit a histogram. The only way I know how to get rid of it is to
    # overwrite it like this.
    c = ROOT.TCanvas(f'c{len(canvas)+1}')
    canvas.append(c)
    h.Draw()
    c.Update()
    if pdf:
        if not filename:
            print('No filename specified; can not print pdf!')
        else:
            root, ext = os.path.splitext(filename)
            c.Print(os.path.join(args.print_pdfs, f"{root}_{h.GetName()}.pdf"))

if __name__ == '__main__':
    from argparse import ArgumentParser
    import ROOT
    from ROOT import gROOT
    import matplotlib.pyplot as plt
    import psycopg2
    import psycopg2.extensions
    import fit_511_funcs
    import fit_spe_funcs

    parser = ArgumentParser(description='Analyze SPE and 511 charges')
    parser.add_argument('filename',help='input filename (hdf5 format)')
    parser.add_argument('-o','--output', default='delete_me.root', help='output file name')
    parser.add_argument('--plot', default=False, action='store_true', help='plot the waveforms and charge integral')
    parser.add_argument('--chunks', default=10000, type=int, help='number of waveforms to process at a time')
    parser.add_argument('-t', '--integration-time', default=300, type=float, help='SPE integration length in nanoseconds.')
    parser.add_argument('-s', '--start-time',  default=50, type=float, help='start time of the SPE integration in nanoseconds.')
    parser.add_argument('--active', default=None, help='Only take data from a single channel. If not specified, all channels are analyzed.')
    parser.add_argument('--integration-method', type=int, default=1, help='Select a method of integration. Methods described in __main__')
    parser.add_argument("--print-pdfs", default=None, type=str, help="Folder to save pdfs in.")
    parser.add_argument('-u','--upload', default=False, action='store_true', help='upload results to the database')
    parser.add_argument('-i','--institution', default=None, type=Institution, choices=list(Institution), help='name of institution')
    args = parser.parse_args()

    if not args.plot:
        # Disables the canvas from ever popping up
        gROOT.SetBatch()

    if args.upload:
        if 'BTL_DB_HOST' not in os.environ:
            print("need to set BTL_DB_HOST environment variable!",file=sys.stderr)
            sys.exit(1)

        if 'BTL_DB_PASS' not in os.environ:
            print("need to set BTL_DB_PASS environment variable!",file=sys.stderr)
            sys.exit(1)

        print("Uploading results to the database...")
        conn = psycopg2.connect(dbname='btl_qa',
                                user='btl',
                                host=os.environ['BTL_DB_HOST'],
                                password=os.environ['BTL_DB_PASS'])
        conn.set_isolation_level(psycopg2.extensions.ISOLATION_LEVEL_AUTOCOMMIT)

        cursor = conn.cursor()

    data = {}
    with h5py.File(args.filename) as f:
        root_f = ROOT.TFile(args.output, "recreate")
                
        if args.upload:
            if 'sodium' not in dict(f):
                print("Missing sodium data!", file=sys.stderr)
                sys.exit(1)
            if 'spe' not in dict(f):
                print("Missing SPE data!", file=sys.stderr)
                sys.exit(1)
            for param in f['sodium'].attrs:
                if f['sodium'].attrs[param] != f['spe'].attrs[param]:
                    print(f"Conflict in {param} used to take sodium and SPE data!", file=sys.stderr)
                    sys.exit(1)
            if 'data_source' in f['sodium'].attrs:
                if f['sodium'].attrs['data_source'] != b'CAEN':
                    print("Error: trying to upload non-CAEN data!", file=sys.stderr)
                    sys.exit(1)
            else:
                print("Data source not specified!", file=sys.stderr)
                sys.exit(1)
            
            data['barcode'] = int(f['sodium'].attrs['barcode'])
            data['voltage'] = int(f['sodium'].attrs['voltage'])
            data['git_sha1'] = f['sodium'].attrs['git_sha1'].decode("UTF-8")
            data['git_dirty'] = f['sodium'].attrs['git_dirty'].decode("UTF-8")
            data['institution'] = str(args.institution)
            data['filename'] = args.filename

            cursor.execute("INSERT INTO runs (voltage, institution, git_sha1, git_dirty, filename) VALUES (%(voltage)s, %(institution)s::inst, %(git_sha1)s, %(git_dirty)s, %(filename)s) RETURNING run", data)
            result = cursor.fetchone()

            run = result[0]
        sodium_charge = {}
        spe_charge = {}
        for group in f:
            if group != 'sodium' and group != 'spe':
                print(f"Unknown group name: \"{group}\". Skipping...")
                continue
            for channel in f[group]:
                # All relevant channels, from the scope and digitizer, should be in
                # this format: 'ch<channel number>'.
                if not channel.startswith('ch'):
                    continue
                
                # Only active channel is analyzed, unless it's `None`, in which
                # case all channels are analyzed.
                if args.active and channel != args.active:
                    continue
                
                charge = []
                f_charge = []
                channel_data = {'channel': int(channel[2:]), 'sodium_rise_time': [], 'sodium_fall_time': []}
                if args.upload:
                    channel_data['run'] = run
                    channel_data['barcode'] = data['barcode']

                ##################
                # Integrations
                ##################
                for i in range(0, len(f[group][channel]), args.chunks):
                    x, y = convert_data(f, group, channel, i, i+args.chunks)
                    if group == 'sodium':
                        a, b = get_window(x,y, left=50, right=125)
                        y -= np.median(y[:,x < x[0] + 100],axis=-1)[:,np.newaxis]
                        
                        channel_data['sodium_rise_time'].extend(get_rise_time(x, y))
                        channel_data['sodium_fall_time'].extend(get_fall_time(x, y))
                    elif group == 'spe':
                        a, b = get_spe_window(x, args.start_time, args.integration_time)
                        y, high_filter_y = spe_baseline_subtraction(x, y, method=args.integration_method)                        
                        f_charge.extend(integrate(x, high_filter_y, a, b))

                    charge.extend(integrate(x,y, a, b))
                        
                    if 'avg_pulse_y' in data:
                        channel_data['avg_pulse_y'] += (data['avg_pulse_count']*data['avg_pulse_y'] + len(y)*np.mean(y, axis=0)) / (data['avg_pulse_count'] + len(y))
                        channel_data['avg_pulse_count'] += len(y)
                    else:
                        channel_data['avg_pulse_y'] = np.mean(y, axis=0)
                        channel_data['avg_pulse_count'] = len(y)
                        channel_data['avg_pulse_x'] = x
                if args.plot or args.print_pdfs:
                    plot_time_volt(x, y, channel, group, a, b, avg_y=channel_data['avg_pulse_y'], pdf=args.print_pdfs)
                    # Plotting the filtered voltage signal. Doesn't have to be included in the final draft of this code.
                    if group == 'spe':
                        plot_time_volt(x, high_filter_y, channel, f"high filter {group}", a, b, pdf=args.print_pdfs)
                
                ##################
                # Creating Histogram
                ##################
                bins = get_bins(charge)
                h = ROOT.TH1D(f"{group}_{channel}", f"{group} Charge Integral for {channel}", len(bins), bins[0], bins[-1])
                for x in charge:
                    h.Fill(x)
                h.GetXaxis().SetTitle("Charge (pC)")
                h.Write()
                if group == 'spe':
                    f_bins = get_bins(f_charge)
                    f_h = ROOT.TH1D(f'f_{channel}', f"Filtered Charge {group} Integral for {channel}", len(f_bins), f_bins[0], f_bins[-1])
                    for x in f_charge:
                        f_h.Fill(x)
                    f_h.GetXaxis().SetTitle('Charge (pC)')
                    f_h.Write()
                
                ##################
                # Fitting Histogram
                ##################
                print(f'Fitting {group} {channel}!')
                if group == 'sodium':
                    sodium_charge[channel] = fit_511_funcs.fit_511(h)
                else:
                    model = fit_spe_funcs.vinogradov_model()
                    spe_charge[channel] = fit_spe_funcs.fit_spe(h, model, f_h=f_h)
                
                plot_hist(h, pdf=args.print_pdfs, filename=args.filename)
                
                ##################
                # Uploading Data per Channel
                ##################
                if args.upload:
                    channel_data['sodium_rise_time'] = float(np.median(channel_data['sodium_rise_time']))
                    channel_data['sodium_fall_time'] = float(np.median(channel_data['sodium_rise_time']))
                    channel_data['avg_pulse_x'] = list(map(float,channel_data['avg_pulse_x']))
                    channel_data['avg_pulse_y'] = list(map(float,channel_data['avg_pulse_y']))
                    # FIXME: testing
                    channel_data['sodium_peak'] = 100
                    channel_data['spe'] = 100

                    bins = get_bins(charge)
                    channel_data['sodium_charge_histogram_y'] = list(map(float,np.histogram(charge,bins=bins)[0]))
                    bincenters = (bins[:1] + bins[:-1])/2
                    channel_data['sodium_charge_histogram_x'] = list(map(float,bincenters))

                    bins = get_bins(charge)
                    channel_data['spe_charge_histogram_y'] = list(map(float,np.histogram(charge,bins=bins)[0]))
                    bincenters = (bins[:1] + bins[:-1])/2
                    channel_data['spe_charge_histogram_x'] = list(map(float,bincenters))

                    result = cursor.execute("INSERT INTO data (channel, barcode, sodium_peak, spe, sodium_rise_time, sodium_fall_time, sodium_charge_histogram_x, sodium_charge_histogram_y, spe_charge_histogram_x, spe_charge_histogram_y, avg_pulse_x, avg_pulse_y, run) VALUES (%(channel)s, %(barcode)s, %(sodium_peak)s, %(spe)s, %(sodium_rise_time)s, %(sodium_fall_time)s, %(sodium_charge_histogram_x)s, %(sodium_charge_histogram_y)s, %(spe_charge_histogram_x)s, %(spe_charge_histogram_y)s, %(avg_pulse_x)s, %(avg_pulse_y)s, %(run)s)", channel_data)
        
        # FIXME: Upload this information to the database
        # If the data is being uploaded, there should have already been a check
        # that all channels have all the needed light output data.
        #
        # If the data is not being uploaded, then we have to preform the checks
        # below. This is to enable testing on just some channels, just using
        # sodium data, etc...
        for i in range(16):
            ch = f'ch{i}'
            if ch not in sodium_charge:
                print(f'Mising sodium data for {ch}!')
            elif ch not in spe_charge:
                print(f'Missing SPE data for {ch}!')
            elif sodium_charge[ch] is None:
                print(f'Failed to fit {ch} sodium histogram!')
            elif spe_charge[ch] is None:
                print(f'Failed to fit {ch} spe histogram!')
            else:
                # sodium_charge[ch][0] is the charge value.
                # sodium_charge[ch][1] is the charge error.
                # Same for spe_charge.
                print(f'{ch}: {sodium_charge[ch][0] / spe_charge[ch][0] / 0.511}')
    
    root_f.Close()

    if args.output == 'delete_me.root':
        os.remove('delete_me.root')

    if args.plot:
        plt.show()

